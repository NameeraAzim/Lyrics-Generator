{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGvO5gx7CjuT"
      },
      "outputs": [],
      "source": [
        "# Import relevant libraries\n",
        "\n",
        "!pip install spotipy\n",
        "\n",
        "# Data augementation\n",
        "!pip install numpy requests nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCuhIlMWd4nv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spotipy\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchtext.legacy import data as torchtext_data\n",
        "\n",
        "# Data augmentation\n",
        "\n",
        "import nlpaug.augmenter.char as nac\n",
        "from nlpaug.util import Action\n",
        "\n",
        "import nlpaug.augmenter.word as naw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bishKU8YC1Ny"
      },
      "outputs": [],
      "source": [
        "# Connect to Spotify\n",
        "\n",
        "cid ='5e9160c5b94c4811ab7c239b3a36a460'\n",
        "secret ='f8288b7c62564f25a5c7d94bd185e896'\n",
        "\n",
        "client_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret)\n",
        "sp = spotipy.Spotify(client_credentials_manager = client_credentials_manager)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXkV9G6l-hyA"
      },
      "outputs": [],
      "source": [
        "# Function to scrape lyrics from Genius\n",
        "\n",
        "def get_album_tracks(uri_info, limit, offset=0):\n",
        "  titles = []\n",
        "  artists = []\n",
        "  one = sp.playlist_tracks(uri_info, limit=limit, offset=offset, market='US')\n",
        "  df1 = pd.DataFrame(one)\n",
        "  for i, x in df1['items'].items():\n",
        "    track = x['track']\n",
        "    titles.append(track['name'])\n",
        "    artists.append(track['artists'][0]['name'])\n",
        "    df2 = pd.DataFrame({'title':titles,'artist':artists})\n",
        "  return df2\n",
        "\n",
        "def scrape_lyrics(artistname, songname):\n",
        "  artistname2 = str(artistname.replace(' ','-')) if ' ' in artistname else str(artistname)\n",
        "  songname2 = str(songname.replace(' ','-')) if ' ' in songname else str(songname)\n",
        "  page = requests.get('https://genius.com/'+ artistname2 + '-' + songname2 + '-' + 'lyrics')\n",
        "  html = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "  lyrics1 = html.find(\"div\", class_=\"lyrics\")\n",
        "  lyrics2 = html.find(\"div\", class_=\"Lyrics__Container-sc-1ynbvzw-6 jYfhrf\")\n",
        "  if lyrics1:\n",
        "    lyrics = lyrics1.get_text()\n",
        "  elif lyrics2:\n",
        "    lyrics = lyrics2.get_text()\n",
        "  elif lyrics1 == lyrics2 == None:\n",
        "    lyrics = None\n",
        "\n",
        "  lines = []\n",
        "  for div in html.findAll('div', {'class': 'Lyrics__Container-sc-1ynbvzw-6 jYfhrf'}):\n",
        "    lines.extend([text if text[0] != '[' else ' ' for text in div.stripped_strings])\n",
        "    \n",
        "  lyrics = \"\"\n",
        "  for line in lines:\n",
        "    if line != ' ':\n",
        "      lyrics += line + '@'  \n",
        "\n",
        "  return lyrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzWaX8pa6Oy-"
      },
      "outputs": [],
      "source": [
        "# Scrape lyrics for the target genre\n",
        "\n",
        "uri = 'spotify:playlist:5JDQq97ipoyekmMG5If3yc'\n",
        "collections = [get_album_tracks(uri, 100, 0), \n",
        "               get_album_tracks(uri, 100, 100), \n",
        "               get_album_tracks(uri, 100, 200), \n",
        "               get_album_tracks(uri, 12, 300)]\n",
        "\n",
        "lyrics = []\n",
        "for df_tracks in collections:\n",
        "  for index, row in df_tracks.iterrows():\n",
        "      song = scrape_lyrics(row['artist'], row['title'])\n",
        "      if song != '':\n",
        "        lyrics.append([song])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiL75U6B_01T"
      },
      "outputs": [],
      "source": [
        "lyrics_copy = lyrics.copy()\n",
        "\n",
        "substitute_aug = nac.RandomCharAug(action=\"substitute\")\n",
        "substitute_lyrics = []\n",
        "for song in lyrics_copy:\n",
        "  lines = song[0].split('@')\n",
        "  aug_song = \"\"\n",
        "  for line in lines:\n",
        "    aug_song += (substitute_aug.augment(line) + \"@\")\n",
        "  substitute_lyrics.append([aug_song])\n",
        "\n",
        "swap_aug = nac.RandomCharAug(action=\"swap\")\n",
        "swap_lyrics = []\n",
        "for song in lyrics_copy:\n",
        "  lines = song[0].split('@')\n",
        "  aug_song = \"\"\n",
        "  for line in lines:\n",
        "    aug_song += (swap_aug.augment(line) + \"@\")\n",
        "  swap_lyrics.append([aug_song])\n",
        "\n",
        "delete_aug = nac.RandomCharAug(action=\"delete\")\n",
        "delete_lyrics = []\n",
        "for song in lyrics_copy:\n",
        "  lines = song[0].split('@')\n",
        "  aug_song = \"\"\n",
        "  for line in lines:\n",
        "    aug_song += (delete_aug.augment(line) + \"@\")\n",
        "  delete_lyrics.append([aug_song])\n",
        "\n",
        "# Word augmentation:\n",
        "\n",
        "def word_aug(action):\n",
        "  aug = None\n",
        "  if action == \"delete\":\n",
        "    aug = naw.RandomWordAug()\n",
        "  else:\n",
        "    aug = naw.RandomWordAug(action)\n",
        "  aug_lyrics = []\n",
        "  for song in lyrics_copy:\n",
        "    lines = song[0].split('@')\n",
        "    aug_song = \"\"\n",
        "    for line in lines:  \n",
        "      aug_song += (aug.augment(line) + \"@\")\n",
        "    aug_lyrics.append([aug_song])\n",
        "  return aug_lyrics\n",
        "\n",
        "swap_word_lyrics = word_aug(\"swap\")\n",
        "delete_word_lyrics = word_aug(\"delete\")\n",
        "\n",
        "split_word_aug = naw.SplitAug()\n",
        "split_word_lyrics = []\n",
        "for song in lyrics_copy:\n",
        "  lines = song[0].split('@')\n",
        "  aug_song = \"\"\n",
        "  for line in lines:\n",
        "    aug_song += (split_word_aug.augment(line) + \"@\")\n",
        "  split_word_lyrics.append([aug_song])\n",
        "\n",
        "\n",
        "lyrics.extend(substitute_lyrics)\n",
        "lyrics.extend(swap_lyrics)\n",
        "lyrics.extend(delete_lyrics)\n",
        "\n",
        "lyrics.extend(swap_word_lyrics)\n",
        "lyrics.extend(delete_word_lyrics)\n",
        "lyrics.extend(split_word_lyrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjoGOvYQae29"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wFUnZrawZ-o",
        "outputId": "4db6d239-2ab3-4f77-9787-1969ba267106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp7evVs3v7i0"
      },
      "outputs": [],
      "source": [
        "import csv \n",
        "\n",
        "f_lyrics = open('/content/drive/MyDrive/csv_lyrics', 'w')\n",
        "writer = csv.writer(f_lyrics)\n",
        "\n",
        "for l in lyrics:\n",
        "  writer.writerow(l)  \n",
        "\n",
        "f_lyrics.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5iWKy54EmqW"
      },
      "outputs": [],
      "source": [
        "f_overfit = open('/content/drive/MyDrive/csv_lyrics_overfit', 'w')\n",
        "writer = csv.writer(f_overfit)\n",
        "\n",
        "writer.writerow([lyrics[1]])  \n",
        "\n",
        "f_overfit.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqWwdc9S7KfI"
      },
      "outputs": [],
      "source": [
        "# Count characters\n",
        "\n",
        "chars = set()\n",
        "for song in lyrics:\n",
        "  for line in song:\n",
        "    temp = list(set(line))\n",
        "    for char in temp:\n",
        "      chars.add(char)\n",
        "\n",
        "chars.add(\"<BOS>\")\n",
        "chars.add(\"<EOS>\")\n",
        "chars = sorted(chars)\n",
        "char_size = len(chars)\n",
        "\n",
        "print(chars)\n",
        "print(\"Number of unique characters: \" + str(char_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpuxzFw4-mlN"
      },
      "outputs": [],
      "source": [
        "# Create dictionary of characters\n",
        "\n",
        "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
        "index_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "print(char_to_index)\n",
        "print(index_to_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eeB3IFuBtO_"
      },
      "outputs": [],
      "source": [
        "# Building the model\n",
        "\n",
        "class SongGeneratorGRU(nn.Module):\n",
        "  def __init__(self, char_size, embedding_size, hidden_size):\n",
        "    super(SongGeneratorGRU, self).__init__()\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embed = nn.Embedding(num_embeddings=char_size, embedding_dim=embedding_size)\n",
        "\n",
        "    # RNN layer\n",
        "    self.rnn = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, batch_first=True)\n",
        "\n",
        "    # Projection MLP layer\n",
        "    self.mlp = nn.Linear(in_features=hidden_size, out_features=char_size)\n",
        "\n",
        "  def forward(self, data, hidden=None):\n",
        "    emb = self.embed(data)\n",
        "    output, hidden = self.rnn(emb, hidden)\n",
        "    output = self.mlp(output)\n",
        "    return output, hidden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3oiQ33MMw0V"
      },
      "outputs": [],
      "source": [
        " # Train with Teacher Forcing\n",
        "\n",
        "def train(model, data, vocab_size, batch_size=1, num_epochs=1, lr=0.001, print_every=100):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    it = 0\n",
        "    \n",
        "    losses, train_acc, valid_acc = [], [], []\n",
        "    epochs = []\n",
        "\n",
        "    data_iter = torchtext_data.BucketIterator(data, \n",
        "                                              batch_size=batch_size,\n",
        "                                              sort_key=lambda x: len(x.text),\n",
        "                                              sort_within_batch=True)\n",
        "    \n",
        "    for e in range(num_epochs):\n",
        "        # get training set\n",
        "        avg_loss = 0\n",
        "        for (lyric, lengths), label in data_iter:\n",
        "            target = lyric[:,1:]\n",
        "            inp = lyric[:,:-1]\n",
        "            # cleanup\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass\n",
        "            output, hidden = model(inp)\n",
        "            loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            avg_loss += loss\n",
        "            losses.append(float(loss)/lyric.size()[0])\n",
        "            it += 1 # increment iteration count\n",
        "            if it % print_every == 0:\n",
        "                print(\"[Iter %d] Loss %f\" % (it+1, float(avg_loss/print_every)))\n",
        "                avg_loss = 0\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EB7B1ZQ6F6_i"
      },
      "outputs": [],
      "source": [
        "text_field_overfit = torchtext_data.Field(sequential=True,      # text sequence\n",
        "                                  tokenize=lambda x: x, # because are building a character-RNN\n",
        "                                  include_lengths=True, # to track the length of sequences, for batching\n",
        "                                  batch_first=True,\n",
        "                                  use_vocab=True,       # to turn each character into an integer index\n",
        "                                  init_token=\"<BOS>\",   # BOS token\n",
        "                                  eos_token=\"<EOS>\")    # EOS token\n",
        "\n",
        "fields_overfit = [('text', text_field_overfit)]\n",
        "lyrics_overfit = torchtext_data.TabularDataset(\"/content/drive/MyDrive/csv_lyrics_overfit\", \"csv\", fields_overfit)\n",
        "text_field_overfit.build_vocab(lyrics_overfit)\n",
        "vocab_stoi_overfit = text_field_overfit.vocab.stoi # so we don't have to rewrite sample_sequence\n",
        "vocab_itos_overfit = text_field_overfit.vocab.itos # so we don't have to rewrite sample_sequence\n",
        "vocab_size_overfit = len(text_field_overfit.vocab.itos)\n",
        "print(vocab_size_overfit)\n",
        "len(lyrics_overfit)\n",
        "\n",
        "model = SongGeneratorGRU(vocab_size_overfit, 256, 256) #char_size, embedding_size, hidden_size\n",
        "train(model, lyrics_overfit, vocab_size_overfit, batch_size=1, num_epochs=60, lr=0.004, print_every=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "az-cAZ86yXFn"
      },
      "outputs": [],
      "source": [
        "text_field = torchtext_data.Field(sequential=True,      # text sequence\n",
        "                                  tokenize=lambda x: x, # because are building a character-RNN\n",
        "                                  include_lengths=True, # to track the length of sequences, for batching\n",
        "                                  batch_first=True,\n",
        "                                  use_vocab=True,       # to turn each character into an integer index\n",
        "                                  init_token=\"<BOS>\",   # BOS token\n",
        "                                  eos_token=\"<EOS>\")    # EOS token\n",
        "\n",
        "fields = [('text', text_field)]\n",
        "lyrics = torchtext_data.TabularDataset(\"/content/drive/MyDrive/csv_lyrics\", \"csv\", fields)\n",
        "text_field.build_vocab(lyrics)\n",
        "vocab_stoi = text_field.vocab.stoi # so we don't have to rewrite sample_sequence\n",
        "vocab_itos = text_field.vocab.itos # so we don't have to rewrite sample_sequence\n",
        "vocab_size = len(text_field.vocab.itos)\n",
        "print(vocab_size)\n",
        "len(lyrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqlVAj9h8lME"
      },
      "outputs": [],
      "source": [
        "model = SongGeneratorGRU(vocab_size, 256, 256) #char_size, embedding_size, hidden_size\n",
        "train(model, lyrics, vocab_size, batch_size=120, num_epochs=95, lr=0.0045, print_every=50) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1yvuI36v_cM"
      },
      "outputs": [],
      "source": [
        "def sample_sequence(model, max_len=100, temperature=0.8):\n",
        "    generated_sequence = \"\"\n",
        "   \n",
        "    inp = torch.Tensor([vocab_stoi[\"<BOS>\"]]).long()\n",
        "    hidden = None\n",
        "    for p in range(max_len):\n",
        "        output, hidden = model(inp.unsqueeze(0), hidden)\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = vocab_itos[top_i]\n",
        "        \n",
        "        if predicted_char == \"<EOS>\":\n",
        "            break\n",
        "        generated_sequence += predicted_char       \n",
        "        inp = torch.Tensor([top_i]).long()\n",
        "    return generated_sequence.replace('@', '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFnVy_p17exO"
      },
      "outputs": [],
      "source": [
        "# LET'S MAKE SOME MUSIC 🎵🎶\n",
        "\n",
        "print(sample_sequence(model, max_len=1500, temperature=0.4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GRU-Model-Data-Augmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}